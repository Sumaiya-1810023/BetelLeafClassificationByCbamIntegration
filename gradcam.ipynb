{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceType":"datasetVersion","sourceId":12558862,"datasetId":7930188},{"sourceType":"modelInstanceVersion","sourceId":493315,"isSourceIdPinned":true}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\n\n# Set dataset path (this must match your Kaggle dataset name)\ndata_path = '/kaggle/input/betel-leaf/Comprehensive Betel Leaf Disease Dataset for Advanced Pathology Research/Betel Leaf Dataset/Betel Leaf Dataset/Betel Leaf Dataset/Augmented_Dataset'  # <- Replace with actual dataset name\n\ncategories = sorted(os.listdir(data_path))\nprint(\"Number of classes:\", categories)\nnoofClasses = len(categories)\nprint(\"Total number of classes:\", noofClasses)\nprint(\"Importing images...\")\n\nlabels = [i for i in range(len(categories))]\nlabel_dict = dict(zip(categories, labels))\n\nprint(\"Label dictionary:\", label_dict)\nprint(\"Categories:\", categories)\nprint(\"Labels:\", labels)\n\nimg_size = 124\ndata = []\ntarget = []\n\nfor category in categories:\n    folder_path = os.path.join(data_path, category)\n    img_names = os.listdir(folder_path)\n\n    for img_name in img_names:\n        img_path = os.path.join(folder_path, img_name)\n        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # Direct grayscale\n\n        try:\n            imga = cv2.resize(img, (img_size, img_size))  # Resize\n\n            # Z-score normalization\n            mean = np.mean(imga)\n            std = np.std(imga)\n            imga = (imga - mean) / (std if std != 0 else 1)\n\n            data.append(imga)\n            target.append(label_dict[category])\n        except Exception as e:\n            print(f'Exception processing image {img_name}: {e}')\n\ndata = np.array(data)\ntarget = np.array(target)\n\nprint(\"Data shape:\", data.shape)\nprint(\"Target shape:\", target.shape)\n\n# Save arrays to Kaggle's writable directory\nnp.save(\"/kaggle/working/FirstStageX.npy\", data)\nnp.save(\"/kaggle/working/FirstStagey.npy\", target)\n\nprint(\" Files saved to /kaggle/working/\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport shutil\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\n# Paths\noriginal_data_dir = '/kaggle/input/betel-leaf/Comprehensive Betel Leaf Disease Dataset for Advanced Pathology Research/Betel Leaf Dataset/Betel Leaf Dataset/Betel Leaf Dataset/Augmented_Dataset'\nbase_output_dir = '/kaggle/working/split_data'\ntrain_dir = os.path.join(base_output_dir, 'train')\nval_dir = os.path.join(base_output_dir, 'val')\ntest_dir = os.path.join(base_output_dir, 'test')\n\n# Create dirs\nfor d in [train_dir, val_dir, test_dir]:\n    os.makedirs(d, exist_ok=True)\n\n# Split each class folder into train/val/test\nclasses = os.listdir(original_data_dir)\n\nfor cls in classes:\n    cls_dir = os.path.join(original_data_dir, cls)\n    images = os.listdir(cls_dir)\n\n    # Shuffle and split\n    train_imgs, test_imgs = train_test_split(images, test_size=0.10, random_state=42)\n    train_imgs, val_imgs = train_test_split(train_imgs, test_size=0.1111, random_state=42)  # 10/90 = 0.1111 for validation (0.10 total)\n\n    for img_name, subset in zip(\n        [train_imgs, val_imgs, test_imgs],\n        [train_dir, val_dir, test_dir]\n    ):\n        subset_cls_dir = os.path.join(subset, cls)\n        os.makedirs(subset_cls_dir, exist_ok=True)\n\n        for img in img_name:\n            src = os.path.join(cls_dir, img)\n            dst = os.path.join(subset_cls_dir, img)\n            shutil.copy(src, dst)\n\nprint(\"✅ Dataset split into train/val/test with 80:10:10 ratio.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T16:33:13.053544Z","iopub.execute_input":"2025-07-28T16:33:13.053846Z","iopub.status.idle":"2025-07-28T16:33:33.291151Z","shell.execute_reply.started":"2025-07-28T16:33:13.053821Z","shell.execute_reply":"2025-07-28T16:33:33.290339Z"}},"outputs":[{"name":"stdout","text":"✅ Dataset split into train/val/test with 80:10:10 ratio.\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport cv2\n\n# --- CBAM attention blocks ---\ndef channel_attention(input_feature, ratio=8):\n    channel = input_feature.shape[-1]\n    shared_dense_one = tf.keras.layers.Dense(channel // ratio, activation='relu')\n    shared_dense_two = tf.keras.layers.Dense(channel)\n    avg_pool = tf.keras.layers.GlobalAveragePooling2D()(input_feature)\n    avg_pool = shared_dense_one(avg_pool)\n    avg_pool = shared_dense_two(avg_pool)\n    max_pool = tf.keras.layers.GlobalMaxPooling2D()(input_feature)\n    max_pool = shared_dense_one(max_pool)\n    max_pool = shared_dense_two(max_pool)\n    cbam_feature = tf.keras.layers.Add()([avg_pool, max_pool])\n    cbam_feature = tf.keras.layers.Activation('sigmoid')(cbam_feature)\n    cbam_feature = tf.keras.layers.Reshape((1, 1, channel))(cbam_feature)\n    return tf.keras.layers.Multiply()([input_feature, cbam_feature])\n\ndef spatial_attention(input_feature):\n    avg_pool = tf.keras.layers.Lambda(\n        lambda x: tf.reduce_mean(x, axis=3, keepdims=True),\n        output_shape=lambda input_shape: (input_shape[0], input_shape[1], input_shape[2], 1)\n    )(input_feature)\n    max_pool = tf.keras.layers.Lambda(\n        lambda x: tf.reduce_max(x, axis=3, keepdims=True),\n        output_shape=lambda input_shape: (input_shape[0], input_shape[1], input_shape[2], 1)\n    )(input_feature)\n    concat = tf.keras.layers.Concatenate(axis=3)([avg_pool, max_pool])\n    cbam_feature = tf.keras.layers.Conv2D(1, kernel_size=7, padding='same', activation='sigmoid')(concat)\n    return tf.keras.layers.Multiply()([input_feature, cbam_feature])\n\ndef cbam_block(input_feature):\n    x = channel_attention(input_feature)\n    x = spatial_attention(x)\n    return x\n\n# --- Model building ---\ndef build_cbam_cnn(input_shape=(224, 224, 3), num_classes=3):\n    inputs = tf.keras.layers.Input(shape=input_shape)\n\n    x = tf.keras.layers.Conv2D(32, 7, padding='same')(inputs)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.ReLU()(x)\n    x = tf.keras.layers.Conv2D(32, 7, padding='same')(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.ReLU()(x)\n    x = cbam_block(x)\n    x = tf.keras.layers.MaxPooling2D()(x)\n\n    x = tf.keras.layers.Conv2D(64, 5, padding='same')(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.ReLU()(x)\n    x = tf.keras.layers.Conv2D(64, 5, padding='same')(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.ReLU()(x)\n    x = cbam_block(x)\n    x = tf.keras.layers.MaxPooling2D()(x)\n\n    x = tf.keras.layers.Conv2D(128, 3, padding='same')(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.ReLU()(x)\n    x = tf.keras.layers.Conv2D(128, 3, padding='same')(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.ReLU()(x)\n    x = cbam_block(x)\n    x = tf.keras.layers.MaxPooling2D()(x)\n\n    x = tf.keras.layers.Conv2D(256, 3, padding='same')(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.ReLU()(x)\n    x = tf.keras.layers.Conv2D(256, 3, padding='same')(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.ReLU()(x)\n    x = cbam_block(x)\n    x = tf.keras.layers.MaxPooling2D()(x)\n\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n    x = tf.keras.layers.Dense(1024, activation='relu')(x)\n    x = tf.keras.layers.Dropout(0.5)(x)\n    x = tf.keras.layers.Dense(512, activation='relu')(x)\n    outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n\n    return tf.keras.models.Model(inputs, outputs)\n\n# Load model weights\nmodel_path = '/kaggle/input/mine/tensorflow2/default/1/betel_leaf_model (1).h5'\nmodel = build_cbam_cnn()\nmodel.load_weights(model_path)\n\n# Assume test_data is your test data generator (without shuffle)\nX_test = []\ny_test = []\nfor i in range(len(test_data)):\n    x_batch, y_batch = test_data[i]\n    X_test.append(x_batch)\n    y_test.append(y_batch)\nX_test = np.vstack(X_test)\ny_test = np.argmax(np.vstack(y_test), axis=1)\n\n# Preprocess input for model\nfrom tensorflow.keras.applications.resnet50 import preprocess_input\nX_test_preprocessed = preprocess_input(X_test.copy())\n\n# Helper function to prepare image for display\ndef prepare_for_display(img):\n    \"\"\"\n    Convert any image tensor to uint8 RGB for display:\n    - If float image (likely normalized), scale to [0,255]\n    - If grayscale (1 channel), repeat to 3 channels\n    - Clip and convert to uint8\n    \"\"\"\n    if img.dtype != np.uint8:\n        img_min = np.min(img)\n        img_max = np.max(img)\n        img = (img - img_min) / (img_max - img_min + 1e-8)  # normalize 0-1\n        img = (img * 255).astype(np.uint8)\n    if img.shape[-1] == 1:\n        img = np.repeat(img, 3, axis=-1)\n    return img\n\n# Find last Conv2D layer\nlast_conv_layer_name = None\nfor layer in reversed(model.layers):\n    if isinstance(layer, tf.keras.layers.Conv2D):\n        last_conv_layer_name = layer.name\n        break\nassert last_conv_layer_name is not None, \"No Conv2D layer found in model.\"\n\nprint(f\"Last Conv2D layer for Grad-CAM: {last_conv_layer_name}\")\n\n# Grad-CAM function\ndef make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n    grad_model = tf.keras.models.Model(\n        [model.inputs],\n        [model.get_layer(last_conv_layer_name).output, model.output]\n    )\n    with tf.GradientTape() as tape:\n        conv_outputs, predictions = grad_model(img_array)\n        if pred_index is None:\n            pred_index = tf.argmax(predictions[0])\n        class_channel = predictions[:, pred_index]\n    grads = tape.gradient(class_channel, conv_outputs)\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n    conv_outputs = conv_outputs[0]\n    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]\n    heatmap = tf.squeeze(heatmap)\n    heatmap = tf.maximum(heatmap, 0) / (tf.reduce_max(heatmap) + 1e-8)\n    return heatmap.numpy()\n\n# Overlay heatmap on image\ndef overlay_heatmap(img, heatmap, alpha=0.4, colormap=cv2.COLORMAP_JET):\n    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n    heatmap = np.uint8(255 * heatmap)\n    heatmap_color = cv2.applyColorMap(heatmap, colormap)\n    if img.dtype != np.uint8:\n        img = prepare_for_display(img)\n    if img.shape[-1] == 1:\n        img = np.repeat(img, 3, axis=-1)\n    if img.shape != heatmap_color.shape:\n        heatmap_color = cv2.resize(heatmap_color, (img.shape[1], img.shape[0]))\n    superimposed_img = cv2.addWeighted(heatmap_color, alpha, img, 1 - alpha, 0)\n    return superimposed_img\n\n# Predict on test set\npreds = model.predict(X_test_preprocessed, verbose=0)\ny_pred = np.argmax(preds, axis=1)\nnum_classes = model.output_shape[-1]\n\n# Get 3 correct samples per class\ncorrect_per_class = {i: [] for i in range(num_classes)}\nfor i in range(len(y_test)):\n    true_label = int(y_test[i])\n    pred_label = int(y_pred[i])\n    if true_label == pred_label and len(correct_per_class[true_label]) < 20:\n        correct_per_class[true_label].append(i)\n    if all(len(v) >= 20 for v in correct_per_class.values()):\n        break\n\n# Plot results\nfor class_id, indices in correct_per_class.items():\n    for idx in indices:\n        original_img = X_test[idx]\n        input_img = np.expand_dims(X_test_preprocessed[idx], axis=0)\n\n        heatmap = make_gradcam_heatmap(input_img, model, last_conv_layer_name)\n        overlay = overlay_heatmap(original_img, heatmap)\n\n        plt.figure(figsize=(12, 4))\n\n        plt.subplot(1, 3, 1)\n        plt.imshow(prepare_for_display(original_img))\n        plt.title(f\"Original Image (Class {class_id})\")\n        plt.axis('off')\n\n        plt.subplot(1, 3, 2)\n        plt.imshow(heatmap, cmap='jet')\n        plt.title(\"Grad-CAM Heatmap\")\n        plt.axis('off')\n\n        plt.subplot(1, 3, 3)\n        plt.imshow(overlay[..., ::-1])  # BGR to RGB for cv2 overlay\n        plt.title(\"Overlay\")\n        plt.axis('off')\n\n        plt.tight_layout()\n        plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}